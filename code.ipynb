{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  torch\n",
    "import argparse\n",
    "# tqdm是进度条库 将range改为trange即可生效\n",
    "# from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "import my_model\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 辅助动图类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        backend_inline.set_matplotlib_formats('svg')\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: self.set_axes(xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def set_axes(self, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置matplotlib的轴\"\"\"\n",
    "        self.axes[0].set_xlabel(xlabel)\n",
    "        self.axes[0].set_ylabel(ylabel)\n",
    "        self.axes[0].set_xscale(xscale)\n",
    "        self.axes[0].set_yscale(yscale)\n",
    "        self.axes[0].set_xlim(xlim)\n",
    "        self.axes[0].set_ylim(ylim)\n",
    "        if legend:\n",
    "            self.axes[0].legend(legend)\n",
    "        self.axes[0].grid()\n",
    "\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "    def show(self):\n",
    "        display.display(self.fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_path, edge_path):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    edge_df = pd.read_csv(edge_path, encoding='utf-8')\n",
    "\n",
    "    # 初始化字典和计数器\n",
    "    geohash_df_dict = {}\n",
    "    date_df_dict = {}\n",
    "    number_hash = 0\n",
    "    number_date = 0\n",
    "\n",
    "    # 为geohash_id创建映射\n",
    "    for i in df[\"geohash_id\"]:\n",
    "        if i not in geohash_df_dict.keys():\n",
    "            geohash_df_dict[i] = number_hash\n",
    "            number_hash += 1\n",
    "\n",
    "    # 为date_id创建映射\n",
    "    for i in df[\"date_id\"]:\n",
    "        if i not in date_df_dict.keys():\n",
    "            date_df_dict[i] = number_date\n",
    "            number_date += 1\n",
    "\n",
    "    # 初始化新数据\n",
    "    new_data = np.zeros((len(date_df_dict), len(geohash_df_dict), 1 + len(df.iloc[0, 2:])))\n",
    "\n",
    "    # 填充新数据\n",
    "    for index, row in df.iterrows():\n",
    "        hash_index, date_index = geohash_df_dict[row[\"geohash_id\"]], date_df_dict[row[\"date_id\"]]\n",
    "        # 将date_index和df.iloc[0, 2:]填充到new_data[date_index][hash_index]位置中\n",
    "        new_data[date_index][hash_index] = np.hstack((date_index, np.array(df.iloc[index, 2:])))\n",
    "    \n",
    "    np.save(\"npdata/new_data.npy\", new_data)\n",
    "\n",
    "    # 构建邻接矩阵\n",
    "    x_mask = np.zeros((len(date_df_dict), len(geohash_df_dict), len(geohash_df_dict), 1), dtype=float)\n",
    "    x_edge_df = np.zeros((len(date_df_dict), len(geohash_df_dict), len(geohash_df_dict), 2), dtype=float)\n",
    "\n",
    "    # 填充邻接矩阵\n",
    "    for index, row in edge_df.iterrows():\n",
    "        if row[\"geohash6_point1\"] not in geohash_df_dict.keys() or row[\"geohash6_point2\"] not in geohash_df_dict.keys():\n",
    "            continue\n",
    "        point1_index, point2_index, F_1, F_2, date_index = geohash_df_dict[row[\"geohash6_point1\"]], geohash_df_dict[row[\"geohash6_point2\"]], row[\"F_1\"], row[\"F_2\"], date_df_dict[row[\"date_id\"]]\n",
    "        x_mask[date_index][point1_index][point2_index] = 1\n",
    "        x_mask[date_index][point2_index][point1_index] = 1\n",
    "        x_edge_df[date_index][point1_index][point2_index] = [F_1, F_2]\n",
    "        x_edge_df[date_index][point2_index][point1_index] = [F_1, F_2]\n",
    "    \n",
    "    np.save(\"npdata/x_mask.npy\", x_mask)\n",
    "    np.save(\"npdata/x_edge_df.npy\", x_edge_df)\n",
    "    \n",
    "    return geohash_df_dict, number_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(file_path, edge_path, pre_geohash_df_dict, pre_number_date):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    edge_df = pd.read_csv(edge_path, encoding='utf-8')\n",
    "\n",
    "    # 初始化字典和计数器\n",
    "    geohash_df_dict = pre_geohash_df_dict\n",
    "    date_df_dict = {}\n",
    "    number_date = pre_number_date\n",
    "    \n",
    "    # 为date_id创建映射\n",
    "    for i in df[\"date_id\"]:\n",
    "        if i not in date_df_dict.keys():\n",
    "            date_df_dict[i] = number_date\n",
    "            number_date += 1\n",
    "\n",
    "    # 初始化新数据\n",
    "    new_data = np.zeros((len(date_df_dict), len(geohash_df_dict), 1 + len(df.iloc[0, 2:])))\n",
    "\n",
    "    # 填充新数据\n",
    "    for index, row in df.iterrows():\n",
    "        hash_index, date_index = geohash_df_dict[row[\"geohash_id\"]], date_df_dict[row[\"date_id\"]]\n",
    "        # 将date_index和df.iloc[0, 2:]填充到new_data[date_index][hash_index]位置中\n",
    "        new_data[date_index - pre_number_date][hash_index] = np.hstack((date_index, np.array(df.iloc[index, 2:])))\n",
    "    \n",
    "    np.save(\"test_npdata/new_data.npy\", new_data)\n",
    "\n",
    "    # 构建邻接矩阵\n",
    "    x_mask = np.zeros((len(date_df_dict), len(geohash_df_dict), len(geohash_df_dict), 1), dtype=float)\n",
    "    x_edge_df = np.zeros((len(date_df_dict), len(geohash_df_dict), len(geohash_df_dict), 2), dtype=float)\n",
    "\n",
    "    # 填充邻接矩阵\n",
    "    for index, row in edge_df.iterrows():\n",
    "        if row[\"geohash6_point1\"] not in geohash_df_dict.keys() or row[\"geohash6_point2\"] not in geohash_df_dict.keys():\n",
    "            continue\n",
    "        point1_index, point2_index, F_1, F_2, date_index = geohash_df_dict[row[\"geohash6_point1\"]], geohash_df_dict[row[\"geohash6_point2\"]], row[\"F_1\"], row[\"F_2\"], date_df_dict[row[\"date_id\"]]\n",
    "        x_mask[date_index - pre_number_date][point1_index][point2_index] = 1\n",
    "        x_mask[date_index - pre_number_date][point2_index][point1_index] = 1\n",
    "        x_edge_df[date_index - pre_number_date][point1_index][point2_index] = [F_1, F_2]\n",
    "        x_edge_df[date_index - pre_number_date][point2_index][point1_index] = [F_1, F_2]\n",
    "\n",
    "    np.save(\"test_npdata/x_mask.npy\", x_mask)\n",
    "    np.save(\"test_npdata/x_edge_df.npy\", x_edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "def eval(model, dataset, args):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dev_loss = 0.0\n",
    "        for j in range(dataset.batch_count):\n",
    "            x_date, x_feature, x_mask_data, x_edge_data, x_tags = dataset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date, x_feature, x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "            loss = criterion(predict, x_tags)\n",
    "            dev_loss += loss\n",
    "        model.train()\n",
    "    return dev_loss.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, x_train, x_mask, x_edge_df):\n",
    "    \n",
    "    if args.rat != 1.0:\n",
    "        # 根据参数比例分割训练集和验证集，并转换为张量\n",
    "        x_train, x_dev = torch.tensor(x_train[:int(len(x_train)*args.rat)]), torch.tensor(x_train[int(len(x_train)*args.rat):])\n",
    "        x_mask_train, x_mask_dev = torch.tensor(x_mask[:int(len(x_mask)*args.rat)]), torch.tensor(x_mask[int(len(x_mask)*args.rat):])\n",
    "        x_edge_train, x_edge_dev = torch.tensor(x_edge_df[:int(len(x_edge_df) * args.rat)]), torch.tensor(x_edge_df[int(len(x_edge_df) * args.rat):])\n",
    "    else:\n",
    "        x_train = torch.tensor(x_train)\n",
    "        x_mask_train = torch.tensor(x_mask)\n",
    "        x_edge_train = torch.tensor(x_edge_df)\n",
    "    # 设置日期嵌入的维度\n",
    "    date_emb = 5\n",
    "    \n",
    "    # 初始化模型\n",
    "    # len(date_df_dict)恒为90\n",
    "    model = my_model.GAT(date_emb=[90, date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "\n",
    "    # 设置优化器\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr)\n",
    "\n",
    "    # 设置模型为训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 创建训练集和验证集的迭代器\n",
    "    trainset = data.DataIterator(x_train, x_mask_train, x_edge_train, args)\n",
    "    if args.rat != 1.0:\n",
    "        valset = data.DataIterator(x_dev, x_mask_dev, x_edge_dev, args)\n",
    "\n",
    "        animator = Animator(xlabel='epoch', xlim=[1, args.epochs], ylim=None,\n",
    "                        legend=['train loss', 'dev loss'])\n",
    "\n",
    "    # 进行多轮训练\n",
    "    for indx in range(args.epochs):\n",
    "        train_all_loss = 0.0\n",
    "\n",
    "        # 对每个批次进行训练\n",
    "        for j in range(trainset.batch_count):\n",
    "            x_date, x_feature, x_mask_data, x_edge_data, x_tags = trainset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date, x_feature, x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(predict, x_tags)\n",
    "            train_all_loss += loss\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.rat != 1.0:\n",
    "            # 绘制训练损失曲线\n",
    "            train_all_loss /= len(x_train)\n",
    "            train_loss = train_all_loss.detach().cpu()\n",
    "            \n",
    "            # 对验证集进行评估\n",
    "            dev_loss = eval(model, valset, args)\n",
    "            dev_loss /= len(x_dev)\n",
    "            \n",
    "            # 打印损失\n",
    "            print('Epoch: {0}, Train loss: {1}, Dev loss: {2}'.format(indx + 1, train_loss, dev_loss))\n",
    "            \n",
    "            # 动态绘制验证损失曲线\n",
    "            animator.add(indx + 1, (train_loss, dev_loss))\n",
    "        else:\n",
    "            # 打印损失\n",
    "            print('Epoch: {0}, Train loss: {1}'.format(indx + 1, train_all_loss / len(x_train)))\n",
    "    \n",
    "    # 返回模型\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_test, x_mask_test, x_edge_test):\n",
    "    \n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"x_mask_test.shape: \", x_mask_test.shape)\n",
    "    print(\"x_edge_test.shape: \", x_edge_test.shape)\n",
    "    \n",
    "    # 转换为张量\n",
    "    x_test, x_mask_test, x_edge_test = torch.FloatTensor(x_test), torch.tensor(x_mask_test), torch.tensor(x_edge_test)\n",
    "    \n",
    "    # 如果设备不匹配，使用to()方法移动张量\n",
    "    if next(model.parameters()).device != x_test.device:\n",
    "        x_test = x_test.to(next(model.parameters()).device)\n",
    "        x_mask_test = x_mask_test.to(next(model.parameters()).device)\n",
    "        x_edge_test = x_edge_test.to(next(model.parameters()).device)\n",
    "    \n",
    "    # 预测\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        act_pre, con_pre = model(x_test[:,:,:1], x_test[:,:,1:], x_mask_test) # 没有用到边数据，所以用两个x_test占位\n",
    "        # 将两个预测结果作为两列并输出为csv文件，并使用当前时间进行标注\n",
    "        predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "        predict = predict.detach().cpu().numpy()\n",
    "        # numpy数组中的第一维是日期，将数据按第一维展开，使得数据变为二维\n",
    "        predict = predict.reshape(-1, predict.shape[-1])\n",
    "        total_predict = pd.DataFrame(predict)\n",
    "        total_predict.to_csv(\"prediction/total_predict.csv\", index=False, header=False)\n",
    "        # 保留最后4*1140行，即保留预测结果\n",
    "        predict = predict[-4*1140:]\n",
    "        predict = pd.DataFrame(predict)\n",
    "        predict.to_csv(\"prediction/predict.csv\", index=False, header=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行数据处理，只需要执行一次，后面只需要读取数据即可\n",
    "# geohash_df_dict, number_date = process_data(\"data/train_90.csv\", \"data/edge_90.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "x_train = np.load(\"train_npdata/new_data.npy\")\n",
    "x_mask = np.load(\"train_npdata/x_mask.npy\")\n",
    "x_edge_df = np.load(\"train_npdata/x_edge_df.npy\")\n",
    "\n",
    "# print(\"x_train.shape: \", x_train.shape)\n",
    "# print(\"x_mask.shape: \", x_mask.shape)\n",
    "# print(\"x_edge_df.shape: \", x_edge_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于检验原来数据处理代码中的bug\n",
    "for i in range(90):\n",
    "    for j in range(1140):\n",
    "        if x_train[i][j][0] != 89.0:\n",
    "            print('xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='training epoch number')\n",
    "parser.add_argument('--batch_size', type=int, default=4,\n",
    "                    help='batch_size')\n",
    "parser.add_argument('--device', type=str, default=\"cuda\",\n",
    "                    help='gpu or cpu')\n",
    "parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                    )\n",
    "parser.add_argument('--rat', type=float, default=0.9,)\n",
    "\n",
    "parser.add_argument('--decline', type=int, default=30, help=\"number of epochs to decline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型参数\n",
    "args = parser.parse_args(['--epochs', '100', '--batch_size', '16', \n",
    "                            '--device', 'cuda', '--lr', '0.001', \n",
    "                            '--rat', '0.9', '--decline', '20'])\n",
    "# 执行训练\n",
    "model = train(args, x_train, x_mask, x_edge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重新训练并进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型在整个数据集进行训练\n",
    "args = parser.parse_args(['--epochs', '20', '--batch_size', '16', \n",
    "                            '--device', 'cuda', '--lr', '0.001', \n",
    "                            '--rat', '1.0', '--decline', '20'])\n",
    "final_model = train(args, x_train, x_mask, x_edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行数据处理，只需要执行一次，后面只需要读取数据即可\n",
    "# process_test_data(\"data/node_test_4_A.csv\", \"data/edge_test_4_A.csv\", geohash_df_dict, number_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行数据处理，只需要执行一次，后面只需要读取数据即可\n",
    "# 合并训练集和测试集，按照第一维合并\n",
    "# x_test = np.concatenate((x_train[:,:,:-2], np.load(\"test_npdata/new_data.npy\")), axis=0)\n",
    "# x_mask_test = np.concatenate((x_mask, np.load(\"test_npdata/x_mask.npy\")), axis=0)\n",
    "# x_edge_test = np.concatenate((x_edge_df, np.load(\"test_npdata/x_edge_df.npy\")), axis=0)\n",
    "\n",
    "# np.save(\"total_test_npdata/x_total_test.npy\", x_test)\n",
    "# np.save(\"total_test_npdata/x_total_mask_test.npy\", x_mask_test)\n",
    "# np.save(\"total_test_npdata/x_total_edge_test.npy\", x_edge_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "temp1 = np.load(\"test_npdata/new_data.npy\")\n",
    "temp2 = np.load(\"test_npdata/x_mask.npy\")\n",
    "temp3 = np.load(\"test_npdata/x_edge_df.npy\")\n",
    "\n",
    "# print(\"temp1.shape: \", temp1.shape)\n",
    "# print(\"temp2.shape: \", temp2.shape)\n",
    "# print(\"temp3.shape: \", temp3.shape)\n",
    "\n",
    "# print(temp1)\n",
    "# print(temp2)\n",
    "# print(temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于检验原来数据处理代码中的bug\n",
    "for i in range(4):\n",
    "    for j in range(1140):\n",
    "        if temp1[i][j][0] != 93.0:\n",
    "            print('xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test.shape:  (94, 1140, 36)\n",
      "x_mask_test.shape:  (94, 1140, 1140, 1)\n",
      "x_edge_test.shape:  (94, 1140, 1140, 2)\n",
      "[[[ 0.000e+00 -7.110e-01 -6.960e-01 ... -4.560e-01 -4.570e-01 -8.300e-01]\n",
      "  [ 0.000e+00 -9.980e-01 -1.037e+00 ... -6.270e-01 -8.290e-01  2.730e-01]\n",
      "  [ 0.000e+00 -1.079e+00 -1.128e+00 ... -6.780e-01 -1.472e+00 -1.332e+00]\n",
      "  ...\n",
      "  [ 0.000e+00 -8.510e-01 -8.910e-01 ... -5.050e-01 -6.680e-01 -9.390e-01]\n",
      "  [ 0.000e+00  6.350e-01  7.130e-01 ...  1.036e+00 -1.219e+00  1.728e+00]\n",
      "  [ 0.000e+00  9.580e-01  8.350e-01 ...  1.212e+00 -4.660e-01  1.170e-01]]\n",
      "\n",
      " [[ 1.000e+00 -9.090e-01 -9.030e-01 ... -5.330e-01  1.130e-01 -8.870e-01]\n",
      "  [ 1.000e+00 -1.039e+00 -1.075e+00 ... -6.390e-01 -6.940e-01 -9.710e-01]\n",
      "  [ 1.000e+00 -1.096e+00 -1.145e+00 ... -6.870e-01 -9.220e-01 -1.875e+00]\n",
      "  ...\n",
      "  [ 1.000e+00 -1.037e+00 -1.073e+00 ... -6.110e-01 -4.960e-01 -2.100e-02]\n",
      "  [ 1.000e+00 -4.610e-01 -3.430e-01 ...  4.240e-01 -5.730e-01  2.941e+00]\n",
      "  [ 1.000e+00 -6.470e-01 -6.410e-01 ...  8.700e-02 -4.340e-01  1.350e-01]]\n",
      "\n",
      " [[ 2.000e+00 -9.200e-01 -9.250e-01 ... -5.400e-01  3.670e-01 -1.021e+00]\n",
      "  [ 2.000e+00 -1.065e+00 -1.104e+00 ... -6.500e-01 -5.770e-01 -1.212e+00]\n",
      "  [ 2.000e+00 -1.100e+00 -1.149e+00 ... -6.810e-01 -8.560e-01  5.510e-01]\n",
      "  ...\n",
      "  [ 2.000e+00 -1.057e+00 -1.091e+00 ... -6.310e-01 -3.010e-01 -9.240e-01]\n",
      "  [ 2.000e+00 -7.290e-01 -6.850e-01 ... -7.400e-02  1.060e-01  2.416e+00]\n",
      "  [ 2.000e+00 -7.250e-01 -7.240e-01 ... -9.000e-03 -1.550e-01  7.240e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 9.100e+01 -7.760e-01 -7.760e-01 ... -4.710e-01 -4.630e-01 -1.686e+00]\n",
      "  [ 9.100e+01 -1.046e+00 -1.090e+00 ... -6.580e-01 -1.298e+00 -2.236e+00]\n",
      "  [ 9.100e+01 -1.038e+00 -1.082e+00 ... -6.510e-01 -1.366e+00 -2.218e+00]\n",
      "  ...\n",
      "  [ 9.100e+01 -6.730e-01 -6.980e-01 ... -2.490e-01 -1.331e+00 -2.200e-02]\n",
      "  [ 9.100e+01  3.260e-01  3.150e-01 ...  6.380e-01 -1.148e+00  9.080e-01]\n",
      "  [ 9.100e+01  2.245e+00  1.940e+00 ...  2.564e+00 -8.340e-01 -1.029e+00]]\n",
      "\n",
      " [[ 9.200e+01 -7.250e-01 -7.170e-01 ... -4.530e-01 -5.690e-01 -1.526e+00]\n",
      "  [ 9.200e+01 -1.047e+00 -1.093e+00 ... -6.560e-01 -1.361e+00 -2.511e+00]\n",
      "  [ 9.200e+01 -1.031e+00 -1.074e+00 ... -6.490e-01 -1.348e+00 -1.956e+00]\n",
      "  ...\n",
      "  [ 9.200e+01 -6.560e-01 -6.770e-01 ... -2.380e-01 -1.306e+00  1.750e-01]\n",
      "  [ 9.200e+01  3.560e-01  3.760e-01 ...  6.220e-01 -1.140e+00  4.400e-01]\n",
      "  [ 9.200e+01  2.154e+00  1.912e+00 ...  2.576e+00 -7.700e-01 -7.090e-01]]\n",
      "\n",
      " [[ 9.300e+01 -7.420e-01 -7.330e-01 ... -4.530e-01 -6.230e-01 -1.511e+00]\n",
      "  [ 9.300e+01 -1.026e+00 -1.078e+00 ... -6.480e-01 -1.414e+00 -2.299e+00]\n",
      "  [ 9.300e+01 -1.006e+00 -1.049e+00 ... -6.330e-01 -1.352e+00 -1.499e+00]\n",
      "  ...\n",
      "  [ 9.300e+01 -6.090e-01 -6.300e-01 ... -2.370e-01 -1.324e+00 -1.200e-01]\n",
      "  [ 9.300e+01  5.660e-01  6.260e-01 ...  7.960e-01 -1.275e+00  2.910e-01]\n",
      "  [ 9.300e+01  2.344e+00  2.181e+00 ...  2.766e+00 -9.100e-01 -9.960e-01]]]\n",
      "[[[[0.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[1.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [1.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n",
      "[[[[0.000e+00 0.000e+00]\n",
      "   [1.200e+01 1.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[1.200e+01 1.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]]\n",
      "\n",
      "\n",
      " [[[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]]\n",
      "\n",
      "\n",
      " [[[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]]\n",
      "\n",
      "\n",
      " [[[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [2.718e+03 1.300e+01]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [2.718e+03 1.300e+01]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]]\n",
      "\n",
      "\n",
      " [[[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]\n",
      "\n",
      "  [[0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   ...\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]\n",
      "   [0.000e+00 0.000e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "x_test = np.load(\"total_test_npdata/x_total_test.npy\")\n",
    "x_mask_test = np.load(\"total_test_npdata/x_total_mask_test.npy\")\n",
    "x_edge_test = np.load(\"total_test_npdata/x_total_edge_test.npy\")\n",
    "\n",
    "# print(\"x_test.shape: \", x_test.shape)\n",
    "# print(\"x_mask_test.shape: \", x_mask_test.shape)\n",
    "# print(\"x_edge_test.shape: \", x_edge_test.shape)\n",
    "\n",
    "# print(x_test)\n",
    "# print(x_mask_test)\n",
    "# print(x_edge_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test.shape:  (94, 1140, 36)\n",
      "x_mask_test.shape:  (94, 1140, 1140, 1)\n",
      "x_edge_test.shape:  (94, 1140, 1140, 2)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 执行预测\n",
    "predict(final_model, x_test, x_mask_test, x_edge_test)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape:  (4560, 37)\n",
      "predict_df.shape:  (4560, 2)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 提取node_test_4_A.csv中的geohash_id和date_id，与prediction/predict.csv中的数据合并\n",
    "# 读取数据\n",
    "test_df = pd.read_csv(\"data/node_test_4_A.csv\", encoding='utf-8')\n",
    "predict_df = pd.read_csv(\"prediction/predict.csv\", encoding='utf-8', header=None)\n",
    "\n",
    "print(\"test_df.shape: \", test_df.shape)\n",
    "print(\"predict_df.shape: \", predict_df.shape)\n",
    "\n",
    "# 合并数据\n",
    "predict_df = pd.concat([test_df.iloc[:, 0], predict_df.iloc[:, 1], predict_df.iloc[:, 0], test_df.iloc[:, 1]], axis=1)\n",
    "\n",
    "# 添加列名\n",
    "predict_df.columns = [\"geohash_id\", \"consumption_level\", \"activity_level\", \"date_id\"] \n",
    "\n",
    "# 保存数据并以当前日期命名\n",
    "\n",
    "# 今日提交次数\n",
    "count = 3\n",
    "\n",
    "# 读取当前日期\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 保存数据\n",
    "predict_df.to_csv(\"submitCSV/submit_\" + now + \"_\" + str(count) +\".csv\", index=False, header=True)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 将文件中的所有逗号替换为tab\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 读取文件夹中的所有文件\n",
    "path = \"submitCSV/\"\n",
    "files = os.listdir(path)\n",
    "\n",
    "# 逐个文件进行处理\n",
    "for file in files:\n",
    "    # 读取文件\n",
    "    with open(path + file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    # 替换所有逗号\n",
    "    data = re.sub(\",\", \"\\t\", data)\n",
    "    # 保存文件\n",
    "    with open(path + file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
